{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05321aee-9064-4d57-87b7-897f8176aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras import backend as K \n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee11d06-728e-4ede-91a0-abede1d796d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to show the min and max values present in the matrices created from image data\n",
    "def show_min_max(array, i):\n",
    "    random_image = array[i]\n",
    "    print(f\"min and max value in image, {random_image.min()}, {random_image.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5142950a-6e49-48cc-9cda-7e0d2c72c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to plot any image in our mnist data \n",
    "def plot_image(array, i, labels):\n",
    "    plt.imshow(np.squeeze(array[i]))\n",
    "    plt.title(\" Digit \" + str(labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea6f841-c1a9-428c-b5bf-bd9f40bc6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dimensions of our data (28x28 pixels per image) (10 classes for numbers 0-9) \n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "num_classes = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda5d565-b729-4cdb-b817-c89303ad72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data into numpy arrays \n",
    "# one array with training data (60,000 images)\n",
    "# one array with testing data (10,000 images)\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n",
    "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928cf65d-7d00-4e79-b877-93cee3b7dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de5f0f3-de75-4038-85f0-d8649cf39740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99655412-3394-44d1-af0e-8e714967a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the previous cells show the shape of the training and test image data \n",
    "# we need to reshape this data to have another dimension, which represents the color \n",
    "# in this example the images are greyscale so the dimension we are adding will be 1 \n",
    "# if the images had color this dimensions value would be 3 (one for each color (red, green blue)) \n",
    "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d7f364-6091-40a7-bd4a-006ce10792c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ebff78-c886-4b6d-b45d-6080c6eacbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets define the shape of each image that will be input into our model \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e51e3e-3c6a-4691-98e9-0a0eee16511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min and max value in image, 0, 255\n"
     ]
    }
   ],
   "source": [
    "# lets take a look at one of the images in our training data (the 100th image)\n",
    "# and see the min and max values (representing pixels) \n",
    "plot_image(train_images, 100, train_labels)\n",
    "show_min_max(train_images, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e9860a-129f-4c48-927b-7baad2f31f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets smake these values more neural network friendly by dividing by the max value (255) to convert them to some float\n",
    "# value between 0 and 1 \n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "885322c3-d24d-46f7-ab47-61fd83e5f544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min and max value in image, 0.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "# now lets check to make sure it worked\n",
    "show_min_max(test_images,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a7a3b1-d235-4f65-8c8e-71f49fd9399d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the format of labels in our data by looking at the first 10 labels in our trainign data labels array\n",
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94915e2e-6bbd-4f00-9aa6-fffee643047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-hot encoding to encode data labels and reduce impact on training and testing \n",
    "# this will convert our individual label values into arrays which reduce the liklihood of our model recognizing patterns in the labels \n",
    "# and overfitting for the training data \n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4df1042-a771-4739-bdd1-0fd606797183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets take another look at the now one-hot encoded labels to see what that encoding looks like visually \n",
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0aec0d9-5a3d-4efc-9de5-ca7267d3e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have all of our data prepared lets make the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03d9f10b-9a1e-497a-86f9-8a3b861d1760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define our sequential model (sequential rows of neurons connected by weights)\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1ac6147-b49d-4cb0-9cb0-912ec64b26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each epoch is one pass over the data while training\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23de3876-c219-4d8f-b0f2-e787f5df26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a layer to the model and pass in the input shape \n",
    "# remember we set input shape equal to (28, 28, 1)\n",
    "# this tells the model that each input is a 28x28 pixel image \n",
    "# the 1 represents the other dimension of measurement for images (Color)\n",
    "# in this case it is 1 because the images are greyscale \n",
    "# if it had color it would be 3 (RED, GREEN and BLUE) \n",
    "model.add(Flatten(input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e22a8ea4-e308-4889-9de0-39c6fc6d66ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our initial layer for each pixel of an image (28x28 = 784)\n",
    "# lets make the inner layers of our network \n",
    "# the first number is the number of neurons in the layer and the activation is the method of condensing the result of the matrix multiplication\n",
    "# into a value that can be repressented by a neuron \n",
    "# for the inner models we will use relu for the activation (on or off) \n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "model.add(Dense(400, activation=\"relu\"))\n",
    "#now lets add the output layer (this will use softmax activation because we are given more info)\n",
    "# softmax is similar to sigmoid and squishes the result to a float between 0 and 1\n",
    "# relu is just (on or off)\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ed3af3-7f25-44cb-9874-ac688b599016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets set up our optimizer \n",
    "# the optimizer is responsible for using gradient desent algorithms to calculate which weights should be adjusted in a certain direction\n",
    "# to minimize the cost functions output (the loss of our model)\n",
    "# the learning rate determines how extreme each weights adjustment will be, as the optimizer attempts to reduce our models loss each iteration\n",
    "learning_rate = 0.00004\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "# we will use categorical crossentropy to calculate loss as this method is ideal for classification models \n",
    "# where the target labels are one-hot encoded\n",
    "# we will also set out metric to accuracy as this is how we will determine the success of our classification model \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c847ced-7612-4563-bff0-60b8ff0bf074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5199 - accuracy: 0.8726\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2249 - accuracy: 0.9372\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1712 - accuracy: 0.9522\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1390 - accuracy: 0.9606\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1156 - accuracy: 0.9681\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0981 - accuracy: 0.9727\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0847 - accuracy: 0.9765\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0738 - accuracy: 0.9794\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0651 - accuracy: 0.9821\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0574 - accuracy: 0.9842\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0510 - accuracy: 0.9861\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0454 - accuracy: 0.9877\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0403 - accuracy: 0.9895\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0356 - accuracy: 0.9908\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0320 - accuracy: 0.9918\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0283 - accuracy: 0.9930\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0252 - accuracy: 0.9939\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0225 - accuracy: 0.9947\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0196 - accuracy: 0.9959\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0175 - accuracy: 0.9962\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0152 - accuracy: 0.9973\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0133 - accuracy: 0.9978\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0117 - accuracy: 0.9980\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0100 - accuracy: 0.9985\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0089 - accuracy: 0.9986\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0076 - accuracy: 0.9990\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0064 - accuracy: 0.9993\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0057 - accuracy: 0.9994\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0049 - accuracy: 0.9995\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0041 - accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "# now that we have our model lets pass in our training data \n",
    "# We use shuffle to reduce the likelihood the the model recognizing patterns\n",
    "# specific to our data so that the resulting \"intelligence\" in generalized well\n",
    "history = model.fit(train_images, train_labels, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68ad0883-5cea-4ffe-9726-a56a93c219c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0682 - accuracy: 0.9816 - 579ms/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# once the training epochs are complete we will do a pass over the test data to see how well generalized our model is \n",
    "# this test data represents how our model will do in the real world as a result of its training\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd4ed3e4-4532-472b-b57a-4b19c28e4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a string to summarize our model's features and evaluation metrics\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_train_accuracy = history.history['accuracy'][-1]\n",
    "learning_rate_str = format(learning_rate, \"f\").rstrip('0').rstrip('.')\n",
    "evaluation = f\"\"\"\n",
    "**Num Epochs:** {epochs}<br>\n",
    "**Training Rate:** {learning_rate_str}<br>\n",
    "**Final Training Accuracy:** {final_train_accuracy*100:.2f}%<br>\n",
    "**Final Training Loss:** {final_train_loss*100:.2f}%<br>\n",
    "**Test Accuracy:** {test_accuracy*100:.2f}%<br>\n",
    "**Test Loss:** {test_loss*100:.2f}%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a218172e-bd81-432a-8bd7-1560d529d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Num Epochs:** 30<br>\n",
       "**Training Rate:** 0.00004<br>\n",
       "**Final Training Accuracy:** 99.96%<br>\n",
       "**Final Training Loss:** 0.41%<br>\n",
       "**Test Accuracy:** 98.16%<br>\n",
       "**Test Loss:** 6.82%\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "114ac891-0aac-4659-8a59-b54fbcfc885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               392500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 400)               200400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                4010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,910\n",
      "Trainable params: 596,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "993d4ad5-7ba9-46ba-81ab-ee94376f7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy notes:\n",
    "\n",
    "#Relu activation inside and softmax for output \n",
    "# adam optimizer \n",
    "\n",
    "# test_1\n",
    "# inner layers: (8,8,4) \n",
    "# epochs: 50\n",
    "# Final Training accuracy: 93.61%\n",
    "# Final Training loss: 23.06%\n",
    "# Test Accuracy: 92.37%\n",
    "# Test Loss: 30.05%\n",
    "\n",
    "# notes: Looks like overfitting so i'm going to try reducing the number of epochs and decrease number of layers\n",
    "\n",
    "#test_2\n",
    "# inner layers: (16, 4)\n",
    "# epochs: 30\n",
    "# Final Training accuracy: 95.85%\n",
    "# Final Training loss: 14.49%\n",
    "# Test Accuracy: 94.54%\n",
    "# Test Loss: 21.08%\n",
    "\n",
    "# I want to play with some parameters so I am going to change the training speed to 90 times as fast \n",
    "# the default training speed in the Adam optimizer is 0.001 so I made is 0.09 for the following test\n",
    "\n",
    "#test_3\n",
    "# Training_speed = 0.009\n",
    "# inner layers: (16, 4)\n",
    "# epochs: 30\n",
    "# Final Training accuracy: 18.86%\n",
    "# Final Training loss: 201.74%\n",
    "# Test Accuracy: 17.16%\n",
    "# Test Loss: 201.43%\n",
    "\n",
    "# Now that we know making the trainng speed faster sucks lets try to make it 10 times slower\n",
    "\n",
    "#test_5\n",
    "# Training_speed = 0.0001\n",
    "# inner layers: (16, 4)\n",
    "# epochs: 30\n",
    "# Final Training accuracy: 92.76%\n",
    "# Final Training loss: 27.04%\n",
    "# Test Accuracy: 92.19%\n",
    "# Test Loss: 29.51%\n",
    "\n",
    "# now we have tested the extremes of training rate lets do an extreme epoch test \n",
    "\n",
    "#test_6\n",
    "# Training_speed = 0.001 (Default)\n",
    "# inner layers: (16, 4)\n",
    "# epochs: 1000\n",
    "# Final Training accuracy: 98.22%\n",
    "# Final Training loss: 6.16%\n",
    "# Test Accuracy: 91.49%\n",
    "# Test Loss: 91.32%\n",
    "\n",
    "#Test_7\n",
    "# Num Epochs: 20\n",
    "# Training Rate: 0.00005\n",
    "# Final Training Accuracy: 99.93%\n",
    "# Final Training Loss: 0.39%\n",
    "# Test Accuracy: 98.21%\n",
    "# Test Loss: 6.92%\n",
    "\n",
    "\n",
    "#https://www.youtube.com/watch?v=NmLK_WQBxB4\n",
    "#https://www.youtube.com/watch?v=pj9-rr1wDhM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
